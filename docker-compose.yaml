# ==========================================================================================================
# docker-compose.yml ‚Äî Airflow 2.9.1 (CeleryExecutor) + Postgres + Redis
# Bind mount APENAS das DAGs (sintaxe longa type: bind) nos servi√ßos runtime + volumes para logs/plugins/DB
# ==========================================================================================================

x-airflow-common: &airflow-common
  image: airflow-custom:latest
  user: "50000:0"
  environment:
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__SQL_ALCHEMY_CONN:      postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__DAG_IGNORE_FILE_REGEX: "(.*/)?(\\.ipynb_checkpoints/.*|__pycache__/.*)|.*(\\.ipynb$|\\-checkpoint\\.py$)"
    PYTHONWARNINGS: "ignore::SyntaxWarning"
    _PIP_ADDITIONAL_REQUIREMENTS: ""
  volumes:
    - airflow-logs-volume:/opt/airflow/logs
    - airflow-plugins-volume:/opt/airflow/plugins
  depends_on:
    postgres:
      condition: service_healthy
    redis:
      condition: service_healthy
  restart: unless-stopped

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow -h localhost"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  # airflow-init usa VOLUME (sem bind) para evitar qualquer mount do Windows nesta fase
  airflow-init:
    <<: *airflow-common
    volumes:
      - airflow-dags-volume:/opt/airflow/dags
      - airflow-logs-volume:/opt/airflow/logs
      - airflow-plugins-volume:/opt/airflow/plugins
    entrypoint: ["bash", "-c"]
    command:
      - >
        set -euo pipefail &&
        echo "üîß [INIT] aguardando Postgres..." &&
        until pg_isready -h postgres -U airflow -d airflow; do sleep 2; done &&
        echo "üîé [INIT] airflow db check (se falhar, roda init)..." &&
        (airflow db check || airflow db init) &&
        echo "‚¨ÜÔ∏è  [INIT] airflow db upgrade..." &&
        airflow db upgrade &&
        echo "üîÅ [INIT] airflow db check (confirmando)..." &&
        airflow db check &&
        echo "üîê [INIT] airflow sync-perm..." &&
        airflow sync-perm || true &&
        echo "üë§ [INIT] criando usu√°rio Admin..." &&
        airflow users create --username airflow --firstname Airflow --lastname Admin --role Admin --email airflow@example.com --password airflow || true &&
        echo "‚úÖ [INIT] conclu√≠do"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: "no"

  # ===== Runtime com bind das DAGs usando SINTAXE LONGA =====
  webserver:
    <<: *airflow-common
    volumes:
      - type: bind
        source: //c/Users/Lucas/Desktop/ENGENHARIA_DE_DADOS/PROJETO_EMPREGADADOS/PROJETO_AIRFLOW/dags
        target: /opt/airflow/dags
        consistency: delegated
      - airflow-logs-volume:/opt/airflow/logs
      - airflow-plugins-volume:/opt/airflow/plugins
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    entrypoint: ["bash", "-c"]
    command: [ "until airflow db check; do echo '‚è≥ aguardando metastore...'; sleep 3; done && exec airflow webserver --port 8080" ]
    ports: ["8080:8080"]
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  scheduler:
    <<: *airflow-common
    volumes:
      - type: bind
        source: //c/Users/Lucas/Desktop/ENGENHARIA_DE_DADOS/PROJETO_EMPREGADADOS/PROJETO_AIRFLOW/dags
        target: /opt/airflow/dags
        consistency: delegated
      - airflow-logs-volume:/opt/airflow/logs
      - airflow-plugins-volume:/opt/airflow/plugins
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    entrypoint: ["bash", "-c"]
    command: [ "until airflow db check; do echo '‚è≥ aguardando metastore...'; sleep 3; done && exec airflow scheduler" ]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5

  worker:
    <<: *airflow-common
    volumes:
      - type: bind
        source: //c/Users/Lucas/Desktop/ENGENHARIA_DE_DADOS/PROJETO_EMPREGADADOS/PROJETO_AIRFLOW/dags
        target: /opt/airflow/dags
        consistency: delegated
      - airflow-logs-volume:/opt/airflow/logs
      - airflow-plugins-volume:/opt/airflow/plugins
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    entrypoint: ["bash", "-c"]
    command: [ "until airflow db check; do echo '‚è≥ aguardando metastore...'; sleep 3; done && exec airflow celery worker" ]
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep 'celery worker' | grep -qv grep"]
      interval: 30s
      timeout: 10s
      retries: 5

  triggerer:
    <<: *airflow-common
    volumes:
      - type: bind
        source: //c/Users/Lucas/Desktop/ENGENHARIA_DE_DADOS/PROJETO_EMPREGADADOS/PROJETO_AIRFLOW/dags
        target: /opt/airflow/dags
        consistency: delegated
      - airflow-logs-volume:/opt/airflow/logs
      - airflow-plugins-volume:/opt/airflow/plugins
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    entrypoint: ["bash", "-c"]
    command: [ "until airflow db check; do echo '‚è≥ aguardando metastore...'; sleep 3; done && exec airflow triggerer" ]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type TriggererJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  postgres-db-volume:
  airflow-dags-volume:
  airflow-logs-volume:
  airflow-plugins-volume:
